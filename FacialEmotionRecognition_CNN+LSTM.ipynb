{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM,Conv2D, MaxPooling2D, Lambda\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVideoFileList(path):\n",
    "    file_list = []\n",
    "    dir_list = os.listdir(path)\n",
    "    \n",
    "    for _dir in dir_list:\n",
    "        files=os.listdir(path+_dir)\n",
    "\n",
    "        for file in files:\n",
    "            if(file.split(\"-\")[0] =='01'):\n",
    "                file_list.append(_dir+\"/\"+file)\n",
    "                \n",
    "    random.shuffle(file_list)\n",
    "    return file_list\n",
    "\n",
    "def getAudioFileListfromVideoFile(path, file_list):\n",
    "    audio_file_list = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        audio_file = file[:9] + '03' + file[11:30] + \"wav\"\n",
    "        audio_file_list.append(audio_file)\n",
    "    \n",
    "    return audio_file_list\n",
    "\n",
    "def getEmotionListfromFileName(file_list):\n",
    "    emotion_list = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        emotion_list.append(int(file.split(\"-\")[2]))\n",
    "    \n",
    "    return emotion_list\n",
    "\n",
    "#video_path = \"../dataset/RAVDESS/Video/\"\n",
    "#audio_path = \"../dataset/RAVDESS/Audio/\"\n",
    "\n",
    "#video_file_list = getVideoFileList(video_path)\n",
    "#audio_file_list = getAudioFileListfromVideoFile(audio_path, video_file_list)\n",
    "#emotion_list = getEmotionListfromFileName(video_file_list)\n",
    "#print(emotion_list)\n",
    "#print(len(video_file_list))\n",
    "#print(len(emotion_list))\n",
    "#print(audio_file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_file_list 불러오기\n",
    "f = open(\"../video_file_list_20200316.txt\", 'r')\n",
    "\n",
    "video_file_list = []\n",
    "\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "        \n",
    "    video_file_list.append(line[:-1])\n",
    "    \n",
    "f.close()\n",
    "\n",
    "video_path = \"../dataset/RAVDESS/Video/\"\n",
    "audio_path = \"../dataset/RAVDESS/Audio/\"\n",
    "audio_file_list = getAudioFileListfromVideoFile(audio_path, video_file_list)\n",
    "emotion_list = getEmotionListfromFileName(video_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 시퀸스 불러오기\n",
    "+ 순차적으로 이미지 불러와 동기를 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/RAVDESS/Images/\"\n",
    "\n",
    "video_frames = []\n",
    "\n",
    "for i in range(len(video_file_list)):\n",
    "    _dir, filename = video_file_list[i].split(\"/\")\n",
    "    \n",
    "    new_dir_name = path+_dir+\"/\"+filename.replace(\".\",\"_\")\n",
    "    #print(new_dir_name)\n",
    "    dir_list = os.listdir(new_dir_name)\n",
    "    \n",
    "    frames = []\n",
    "    for j in range(len(dir_list)):\n",
    "        file = str(j)+\".jpg\"\n",
    "        gray = cv2.imread(new_dir_name + \"/\" + file, cv2.IMREAD_GRAYSCALE) / 255.0\n",
    "        \n",
    "        frames.append(gray)\n",
    "    \n",
    "    video_frames.append(frames) \n",
    "#print(np.shape(video_frames[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmark 시퀸스 불러오기\n",
    "+ 순차적으로 이미지를 불러와 동기를 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/RAVDESS/Landmarks/\"\n",
    "\n",
    "face_landmarks=[]\n",
    "for i in range(len(video_file_list)):\n",
    "    _dir, filename = video_file_list[i].split(\"/\")\n",
    "    \n",
    "    new_dir_name = path+_dir+\"/\"+filename.replace(\".\",\"_\")\n",
    "    \n",
    "    dir_list = os.listdir(new_dir_name)\n",
    "    \n",
    "    frames=[]\n",
    "    for j in range(len(dir_list)):\n",
    "        file = str(j)+\".txt\"\n",
    "        \n",
    "        f= open(new_dir_name+\"/\"+file, \"r\")\n",
    "        landmark = []\n",
    "        \n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: \n",
    "                break\n",
    "                \n",
    "            if line != \"\":\n",
    "                landmark.append(int(line))\n",
    "                \n",
    "        frames.append(landmark)\n",
    "        \n",
    "    face_landmarks.append(frames)\n",
    "#print(face_landmarks[0][0]) #\n",
    "\n",
    "#pre-processing\n",
    "for i in range(len(face_landmarks)):\n",
    "    for j in range(len(face_landmarks[i])):\n",
    "        x_s = np.array(face_landmarks[i][j][0::2])\n",
    "        y_s = np.array(face_landmarks[i][j][1::2])\n",
    "        \n",
    "        x_o = x_s[16]\n",
    "        y_o = y_s[16]\n",
    "        \n",
    "        x_std = np.std(x_s)\n",
    "        y_std = np.std(y_s)\n",
    "        \n",
    "        for k in range(0, len(face_landmarks[i][j]), 2):\n",
    "            face_landmarks[i][j][k] = (face_landmarks[i][j][k] - x_o) /x_std\n",
    "            \n",
    "        for k in range(1, len(face_landmarks[i][j]), 2):\n",
    "            face_landmarks[i][j][k] = (face_landmarks[i][j][k] - y_o) /y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 48000\n",
    "window_size = 1600\n",
    "\n",
    "path = \"../dataset/RAVDESS/Audio_Features/\"\n",
    "\n",
    "audio_features = []\n",
    "for i in range(len(audio_file_list)):\n",
    "    _dir, filename = audio_file_list[i].split(\"/\")\n",
    "    \n",
    "    new_dir_name = path+_dir+\"/\"+filename.replace(\".\",\"_\")\n",
    "    \n",
    "    dir_list = os.listdir(new_dir_name)\n",
    "    \n",
    "    frames=[]\n",
    "    for j in range(len(dir_list)):\n",
    "        file = str(j)+\".txt\"\n",
    "        \n",
    "        f= open(new_dir_name+\"/\"+file, \"r\")\n",
    "        features = []\n",
    "        \n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: \n",
    "                break\n",
    "                \n",
    "            if line != \"\":\n",
    "                features.append(float(line))\n",
    "                \n",
    "        frames.append(features)\n",
    "        \n",
    "    audio_features.append(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter setting\n",
    "class_size =8 #neutral, calm, happy, sad, angry, fearful, disgust, surprised\n",
    "time_step = 10\n",
    "sliding_value = int(time_step/2)\n",
    "image_x = []\n",
    "landmark_x = []\n",
    "audio_x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지 sequence 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 sequence 데이터\n",
    "for i in range(len(video_frames)):\n",
    "    for j in range(0, len(video_frames[i]) - time_step + 1, sliding_value): #j는 시간\n",
    "        temp = video_frames[i][j:j+time_step]\n",
    "        image_x.append(temp)\n",
    "        y.append(emotion_list[i])\n",
    "#print(np.shape(image_x), np.shape(y)) #((데이터길이), 10(time_steps), 64, 64(64*64이미지)) (3273, )\n",
    "\n",
    "X_train = np.array(image_x)\n",
    "X_train = X_train.reshape(len(X_train),time_step,64,64,1)\n",
    "Y_train = np.array(y)\n",
    "Y_train = label_binarize(Y_train, classes = [1, 2, 3, 4, 5, 6, 7, 8])\n",
    "#print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Landmark 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13838, 10, 98) (13838, 8)\n"
     ]
    }
   ],
   "source": [
    "#Landmark 데이터\n",
    "for i in range(len(face_landmarks)):\n",
    "    for j in range(0, len(face_landmarks[i])- time_step + 1, sliding_value):\n",
    "        temp = face_landmarks[i][j:j+time_step]\n",
    "        landmark_x.append(temp)\n",
    "        y.append(emotion_list[i])\n",
    "\n",
    "Landmark_X_train = []\n",
    "\n",
    "#for i in range(len(landmark_x)):\n",
    "#    x = []\n",
    "#    for j in range(len(landmark_x[i])):\n",
    "#        x.extend(landmark_x[i][j])\n",
    "        \n",
    "#    Landmark_X_train.append(x)\n",
    "    \n",
    "Landmark_X_train = np.array(landmark_x)\n",
    "print(np.shape(Landmark_X_train), np.shape(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speech 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13838, 10, 43)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(audio_features)):\n",
    "    for j in range(0, len(audio_features[i])- time_step + 1, sliding_value):\n",
    "        temp = audio_features[i][j:j+time_step]\n",
    "        audio_x.append(temp)\n",
    "        y.append(emotion_list[i])\n",
    "\n",
    "Audio_X_train = np.array(audio_x)\n",
    "print(np.shape(Audio_X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+LSTM model\n",
    "+ video_frames -> CNN -> featuremap\n",
    "+ output_featuremap + face_landmarks -> LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13838, 10, 64, 64, 1) (13838, 10, 98) (13838, 10, 43) (13838, 8)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(X_train), np.shape(Landmark_X_train), np.shape(Audio_X_train), np.shape(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_train[:1300])\n",
    "X_train = np.array(X_train[1300:])\n",
    "Landmark_X_test = np.array(Landmark_X_train[:1300])\n",
    "Landmark_X_train = np.array(Landmark_X_train[1300:])\n",
    "Audio_X_test = np.array(Audio_X_train[:1300])\n",
    "Audio_X_train = np.array(Audio_X_train[1300:])\n",
    "Y_test = np.array(Y_train[:1300])\n",
    "Y_train = np.array(Y_train[1300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12538, 10, 64, 64, 1) (1300, 10, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(X_train),np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12538, 10, 98) (1300, 10, 98)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(Landmark_X_train), np.shape(Landmark_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12538, 10, 43) (1300, 10, 43)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(Audio_X_train),np.shape(Audio_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12538, 8) (1300, 8)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(Y_train), np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z normalization(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 43\n",
    "\n",
    "x_for_scaling = []\n",
    "avg = []\n",
    "std = []\n",
    "\n",
    "#training feature를 feature별로\n",
    "for i in range(feature_size):\n",
    "    temp = []\n",
    "    x_for_scaling.append(temp)\n",
    "    \n",
    "for i in range(len(Audio_X_train)):\n",
    "    for j in range(len(Audio_X_train[i])):\n",
    "        for k in range(len(Audio_X_train[i][j])):\n",
    "            x_for_scaling[k].append(Audio_X_train[i][j][k])\n",
    "            \n",
    "#각 feature의 평균과 표준편차를 구함\n",
    "for i in range(len(x_for_scaling)):\n",
    "    avg.append(np.mean(x_for_scaling[i]))\n",
    "    std.append(np.std(x_for_scaling[i]))\n",
    "    \n",
    "#training 벡터 normalization\n",
    "for i in range(len(Audio_X_train)):\n",
    "    for j in range(len(Audio_X_train[i])):\n",
    "        for k in range(len(Audio_X_train[i][j])):\n",
    "            Audio_X_train[i][j][k] = (Audio_X_train[i][j][k] - avg[k])/std[k]\n",
    "            \n",
    "#test 벡터 normalization\n",
    "for i in range(len(Audio_X_test)):\n",
    "    for j in range(len(Audio_X_test[i])):\n",
    "        for k in range(len(Audio_X_test[i][j])):\n",
    "            Audio_X_test[i][j][k] = (Audio_X_test[i][j][k] - avg[k])/std[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pmp/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "12538/12538 [==============================] - 8s 619us/step - loss: 2.0056 - accuracy: 0.1924\n",
      "Epoch 2/50\n",
      "12538/12538 [==============================] - 6s 444us/step - loss: 1.6905 - accuracy: 0.3806\n",
      "Epoch 3/50\n",
      "12538/12538 [==============================] - 6s 446us/step - loss: 1.2818 - accuracy: 0.5483\n",
      "Epoch 4/50\n",
      "12538/12538 [==============================] - 6s 444us/step - loss: 1.0124 - accuracy: 0.6491\n",
      "Epoch 5/50\n",
      "12538/12538 [==============================] - 6s 446us/step - loss: 0.8065 - accuracy: 0.7335\n",
      "Epoch 6/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.6440 - accuracy: 0.7932\n",
      "Epoch 7/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.4967 - accuracy: 0.8460\n",
      "Epoch 8/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.4424 - accuracy: 0.8643\n",
      "Epoch 9/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.3245 - accuracy: 0.9022\n",
      "Epoch 10/50\n",
      "12538/12538 [==============================] - 6s 445us/step - loss: 0.2651 - accuracy: 0.9217\n",
      "Epoch 11/50\n",
      "12538/12538 [==============================] - 6s 445us/step - loss: 0.2435 - accuracy: 0.9278\n",
      "Epoch 12/50\n",
      "12538/12538 [==============================] - 6s 446us/step - loss: 0.1901 - accuracy: 0.9474\n",
      "Epoch 13/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.1928 - accuracy: 0.9429\n",
      "Epoch 14/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.1670 - accuracy: 0.9519\n",
      "Epoch 15/50\n",
      "12538/12538 [==============================] - 6s 446us/step - loss: 0.1542 - accuracy: 0.9573\n",
      "Epoch 16/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.1273 - accuracy: 0.9651\n",
      "Epoch 17/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.1137 - accuracy: 0.9673\n",
      "Epoch 18/50\n",
      "12538/12538 [==============================] - 6s 446us/step - loss: 0.1008 - accuracy: 0.9714\n",
      "Epoch 19/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.1009 - accuracy: 0.9714\n",
      "Epoch 20/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0969 - accuracy: 0.9724\n",
      "Epoch 21/50\n",
      "12538/12538 [==============================] - 6s 446us/step - loss: 0.1651 - accuracy: 0.9533\n",
      "Epoch 22/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0713 - accuracy: 0.9784\n",
      "Epoch 23/50\n",
      "12538/12538 [==============================] - 6s 446us/step - loss: 0.0581 - accuracy: 0.9830\n",
      "Epoch 24/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0871 - accuracy: 0.9764\n",
      "Epoch 25/50\n",
      "12538/12538 [==============================] - 6s 446us/step - loss: 0.0731 - accuracy: 0.9801\n",
      "Epoch 26/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0690 - accuracy: 0.9800\n",
      "Epoch 27/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0668 - accuracy: 0.9805\n",
      "Epoch 28/50\n",
      "12538/12538 [==============================] - 6s 445us/step - loss: 0.0911 - accuracy: 0.9738\n",
      "Epoch 29/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0565 - accuracy: 0.9836\n",
      "Epoch 30/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0382 - accuracy: 0.9889\n",
      "Epoch 31/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0489 - accuracy: 0.9864\n",
      "Epoch 32/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0690 - accuracy: 0.9798\n",
      "Epoch 33/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0822 - accuracy: 0.9762\n",
      "Epoch 34/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0626 - accuracy: 0.9829\n",
      "Epoch 35/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0801 - accuracy: 0.9770\n",
      "Epoch 36/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0681 - accuracy: 0.9802\n",
      "Epoch 37/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0357 - accuracy: 0.9902\n",
      "Epoch 38/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0177 - accuracy: 0.9941\n",
      "Epoch 39/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0740 - accuracy: 0.9811\n",
      "Epoch 40/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0567 - accuracy: 0.9833\n",
      "Epoch 41/50\n",
      "12538/12538 [==============================] - 6s 449us/step - loss: 0.0458 - accuracy: 0.9878\n",
      "Epoch 42/50\n",
      "12538/12538 [==============================] - 6s 449us/step - loss: 0.0528 - accuracy: 0.9848\n",
      "Epoch 43/50\n",
      "12538/12538 [==============================] - 6s 449us/step - loss: 0.0327 - accuracy: 0.9900\n",
      "Epoch 44/50\n",
      "12538/12538 [==============================] - 6s 449us/step - loss: 0.0772 - accuracy: 0.9779\n",
      "Epoch 45/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0439 - accuracy: 0.9859\n",
      "Epoch 46/50\n",
      "12538/12538 [==============================] - 6s 449us/step - loss: 0.0419 - accuracy: 0.9868\n",
      "Epoch 47/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0421 - accuracy: 0.9879\n",
      "Epoch 48/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0270 - accuracy: 0.9903\n",
      "Epoch 49/50\n",
      "12538/12538 [==============================] - 6s 448us/step - loss: 0.0291 - accuracy: 0.9907\n",
      "Epoch 50/50\n",
      "12538/12538 [==============================] - 6s 447us/step - loss: 0.0857 - accuracy: 0.9769\n",
      "1300/1300 [==============================] - 1s 746us/step\n",
      "0.7699999809265137\n"
     ]
    }
   ],
   "source": [
    "input_x = layers.Input(shape = (time_step, 64, 64,1))\n",
    "input_x_landmark = layers.Input(shape = (time_step, 98))\n",
    "input_x_audio = layers.Input(shape = (time_step, 43))\n",
    "\n",
    "stack = []    \n",
    "\n",
    "for i in range(time_step):\n",
    "    in_layer = Lambda(lambda x : x[:,i,:,:,:]) (input_x)\n",
    "    conv_layer1 = layers.Conv2D(8,(3,3), activation = \"relu\")(in_layer)\n",
    "    pooling1 = layers.MaxPooling2D((2,2))(conv_layer1)\n",
    "    conv_layer2 = layers.Conv2D(16,(3,3), activation = \"relu\")(pooling1)\n",
    "    pooling2 = layers.MaxPooling2D((2,2))(conv_layer2) #shape = (데이터길이, 14,14,16)\n",
    "\n",
    "    flat = layers.Flatten()(pooling2)\n",
    "    stack.append(flat)\n",
    "\n",
    "\n",
    "lstm_in1 = Lambda(lambda x : tf.stack(x,axis=1))(stack) #shape = (?,10,? )\n",
    "lstm_in2 = Lambda(lambda x : tf.concat((x,input_x_landmark),2))(lstm_in1)\n",
    "lstm_in3 = Lambda(lambda x : tf.concat((x,input_x_audio),2))(lstm_in2)\n",
    "lstm1 = layers.LSTM(64, input_shape = (time_step, 14*14*16+98+43))(lstm_in3) #lstm_ 입력 shape = datasize, timestep, 14*14*16+98+43\n",
    "#lstm2 = layers.LSTM(64)(lstm1) return_sequences = True,\n",
    "dense1 = layers.Dense(32, activation='relu')(lstm1)\n",
    "dropout1 = layers.Dropout(0.5)(dense1)\n",
    "dense2 = layers.Dense(class_size)(dropout1)\n",
    "lstm_out = layers.Softmax()(dense2)\n",
    "\n",
    "model = Model([input_x, input_x_landmark, input_x_audio], lstm_out)\n",
    "model.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "###\n",
    "#model.fit(X_train, Y_train, epochs = 50, batch_size = 100)\n",
    "model.fit([X_train, Landmark_X_train, Audio_X_train], Y_train, epochs = 50, batch_size = 100)\n",
    "\n",
    "avg_acc=0.0\n",
    "acc = model.evaluate([X_test, Landmark_X_test, Audio_X_test], Y_test)\n",
    "#avg_acc+=acc[1]\n",
    "print(acc[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ video_file_list_20200316 => 0.76\n",
    "+ video_file_list_20200318 => 0.77\n",
    "+ video_file_list_20200319 => 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'max_pooling2d_20/MaxPool:0' shape=(?, 14, 14, 16) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a=tf.concat([stack,input_x_landmark],axis=1)\n",
    "pooling2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'flatten_1/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_2/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_3/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_4/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_5/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_6/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_7/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_8/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_9/Reshape:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'flatten_10/Reshape:0' shape=(?, ?) dtype=float32>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda_11/stack:0' shape=(?, 10, ?) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_in1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda_12/concat:0' shape=(?, 10, ?) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_in2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat_1:0\", shape=(?, 10, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a =tf.concat((lstm_in1,input_x_landmark),2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
